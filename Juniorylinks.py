import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime

# Configuraci√≥n
BASE_URL = "https://www.coretennis.net"
WEEKLY_URLS = [
    "https://www.coretennis.net/majic/pageServer/1z0100000y/en/Junior-Girls.html",
]
# Lista de URLs de torneos espec√≠ficos para incluir manualmente
SPECIFIC_TOURNAMENT_URLS = [
"https://www.coretennis.net/majic/pageServer/0r0100000c/en/tid/121056/Tournament-Rounds.html",
"https://www.coretennis.net/majic/pageServer/0r0100000c/en/tid/121057/Tournament-Rounds.html",
]
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "es-ES,es;q=0.9"
}

# Mapeo de n√∫meros de mes a nombres en espa√±ol
MONTH_NAMES = {
    1: "Enero", 2: "Febrero", 3: "Marzo", 4: "Abril",
    5: "Mayo", 6: "Junio", 7: "Julio", 8: "Agosto",
    9: "Septiembre", 10: "Octubre", 11: "Noviembre", 12: "Diciembre"
}

SESSION = requests.Session()
SESSION.headers.update(HEADERS)

def get_country_flag(country_code):
    flag_emojis = {
    'USA': 'üá∫üá∏', 'ARG': 'üá¶üá∑', 'COL': 'üá®üá¥', 'FRA': 'üá´üá∑', 
    'ITA': 'üáÆüáπ', 'ESP': 'üá™üá∏', 'SUI': 'üá®üá≠', 'JPN': 'üáØüáµ',
    'RUS': 'üá∑üá∫', 'GER': 'üá©üá™', 'BRA': 'üáßüá∑', 'CHN': 'üá®üá≥',
    'AUS': 'üá¶üá∫', 'GBR': 'üá¨üáß', 'CAN': 'üá®üá¶', 'BEL': 'üáßüá™',
    'NED': 'üá≥üá±', 'SWE': 'üá∏üá™', 'CZE': 'üá®üáø', 'SRB': 'üá∑üá∏',
    'CRO': 'üá≠üá∑', 'UKR': 'üá∫üá¶', 'ROU': 'üá∑üá¥', 'POL': 'üáµüá±',
    'SVK': 'üá∏üá∞', 'KAZ': 'üá∞üáø', 'BLR': 'üáßüáæ', 'HUN': 'üá≠üá∫',
    'GRE': 'üá¨üá∑', 'TUR': 'üáπüá∑', 'ISR': 'üáÆüá±', 'RSA': 'üáøüá¶',
    'SLO': 'üá∏üáÆ', 'BUL': 'üáßüá¨', 'LAT': 'üá±üáª', 'EST': 'üá™üá™',
    'TUN': 'üáπüá≥', 'CYP': 'üá®üáæ', 'EGY': 'üá™üá¨', 'UNK': 'üè≥Ô∏è',
    'LTU': 'üá±üáπ', 'IND': 'üáÆüá≥', 'THA': 'üáπüá≠', 'KOR': 'üá∞üá∑',
    'MDA': 'üá≤üá©', 'AUT': 'üá¶üáπ', 'GEO': 'üá¨üá™', 'MEX': 'üá≤üáΩ',
    'PER': 'üáµüá™', 'CHI': 'üá®üá±', 'VEN': 'üáªüá™', 'ECU': 'üá™üá®',
    'BOL': 'üáßüá¥', 'PRY': 'üáµüáæ', 'URY': 'üá∫üáæ', 'PRT': 'üáµüáπ',
    'NOR': 'üá≥üá¥', 'FIN': 'üá´üáÆ', 'DEN': 'üá©üá∞', 'IRL': 'üáÆüá™',
    'ISL': 'üáÆüá∏', 'MAR': 'üá≤üá¶', 'ALG': 'üá©üáø', 'NGA': 'üá≥üá¨',
    'KEN': 'üá∞üá™', 'PHL': 'üáµüá≠', 'SGP': 'üá∏üá¨', 'MYS': 'üá≤üáæ',
    'IDN': 'üáÆüá©', 'PAK': 'üáµüá∞', 'BGD': 'üáßüá©', 'NPL': 'üá≥üáµ',
    'LKA': 'üá±üá∞', 'UAE': 'üá¶üá™', 'QAT': 'üá∂üá¶', 'SAU': 'üá∏üá¶',
    'NZL': 'üá≥üáø', 'ZAF': 'üáøüá¶', 'CMR': 'üá®üá≤', 'GHA': 'üá¨üá≠',
    'SEN': 'üá∏üá≥', 'CIV': 'üá®üáÆ', 'UGA': 'üá∫üá¨', 'TZA': 'üáπüáø',
    'ZWE': 'üáøüáº', 'MOZ': 'üá≤üáø', 'ANG': 'üá¶üá¥', 'NAM': 'üá≥üá¶',
    'JAM': 'üáØüá≤', 'DOM': 'üá©üá¥', 'PRI': 'üáµüá∑', 'CUB': 'üá®üá∫',
    'HTI': 'üá≠üáπ', 'PAN': 'üáµüá¶', 'CRI': 'üá®üá∑', 'NIC': 'üá≥üáÆ',
    'HND': 'üá≠üá≥', 'SLV': 'üá∏üáª', 'GTM': 'üá¨üáπ', 'BLZ': 'üáßüáø',
    'MNE': 'üá≤üá™', 'LBN': 'üá±üáß', 'JOR': 'üáØüá¥', 'IRQ': 'üáÆüá∂',
    'SYR': 'üá∏üáæ', 'YEM': 'üáæüá™', 'OMN': 'üá¥üá≤', 'KWT': 'üá∞üáº',
    'BHR': 'üáßüá≠', 'ARM': 'üá¶üá≤', 'AZE': 'üá¶üáø', 'KGZ': 'üá∞üá¨',
    'TJK': 'üáπüáØ', 'UZB': 'üá∫üáø', 'TKM': 'üáπüá≤', 'AFG': 'üá¶üá´',
    'IRN': 'üáÆüá∑', 'PSE': 'üáµüá∏', 'LBY': 'üá±üáæ', 'SDN': 'üá∏üá©',
    'ETH': 'üá™üáπ', 'SOM': 'üá∏üá¥', 'DJI': 'üá©üáØ', 'ERI': 'üá™üá∑',
    'MDV': 'üá≤üáª', 'BTN': 'üáßüáπ', 'MMR': 'üá≤üá≤', 'LAO': 'üá±üá¶',
    'KHM': 'üá∞üá≠', 'VNM': 'üáªüá≥', 'TLS': 'üáπüá±', 'BRN': 'üáßüá≥',
    'PNG': 'üáµüá¨', 'FJI': 'üá´üáØ', 'SLB': 'üá∏üáß', 'VUT': 'üáªüá∫',
    'WSM': 'üáºüá∏', 'TON': 'üáπüá¥', 'KIR': 'üá∞üáÆ', 'PLW': 'üáµüáº',
    'FSM': 'üá´üá≤', 'MHL': 'üá≤üá≠', 'NRU': 'üá≥üá∑', 'TUV': 'üáπüáª',
    'DMA': 'üá©üá≤', 'GRD': 'üá¨üá©', 'LCA': 'üá±üá®', 'VCT': 'üáªüá®',
    'KNA': 'üá∞üá≥', 'ATG': 'üá¶üá¨', 'BRB': 'üáßüáß', 'LSO': 'üá±üá∏',
    'SWZ': 'üá∏üáø', 'MWI': 'üá≤üáº', 'COD': 'üá®üá©', 'COG': 'üá®üá¨',
    'CAF': 'üá®üá´', 'TCD': 'üáπüá©', 'GNQ': 'üá¨üá∂', 'GAB': 'üá¨üá¶',
    'BEN': 'üáßüáØ', 'TGO': 'üáπüá¨', 'MLI': 'üá≤üá±', 'BFA': 'üáßüá´',
    'NER': 'üá≥üá™', 'MRT': 'üá≤üá∑', 'GNB': 'üá¨üáº', 'CPV': 'üá®üáª',
    'STP': 'üá∏üáπ', 'SYC': 'üá∏üá®', 'COM': 'üá∞üá≤', 'MUS': 'üá≤üá∫',
    'LIE': 'üá±üáÆ', 'SMR': 'üá∏üá≤', 'AND': 'üá¶üá©', 'MCO': 'üá≤üá®',
    'VAT': 'üáªüá¶', 'MAF': 'üá≤üá´', 'BLM': 'üáßüá±', 'SPM': 'üáµüá≤',
    'WLF': 'üáºüá´', 'PYF': 'üáµüá´', 'NCL': 'üá≥üá®', 'COK': 'üá®üá∞',
    'TKL': 'üáπüá∞', 'NIU': 'üá≥üá∫', 'PCN': 'üáµüá≥', 'HMD': 'üá≠üá≤',
    'ATA': 'üá¶üá∂', 'BVT': 'üáßüáª', 'IOT': 'üáÆüá¥', 'ATF': 'üáπüá´',
    'UMI': 'üá∫üá≤', 'FLK': 'üá´üá∞', 'SGS': 'üá¨üá∏', 'ESH': 'üá™üá≠',
    }
    return flag_emojis.get(country_code.upper(), 'üè≥Ô∏è')

def clean_tournament_name(name):
    """Extrae solo el nombre b√°sico del torneo para el archivo de salida"""
    name = re.sub(r'\(.*?\)', '', name)
    name = re.sub(r',\s*\d{4}-\d{2}-\d{2}', '', name)
    name = re.sub(r'Tennis Results$', '', name)
    name = re.sub(r'\s+', ' ', name).strip()
    
    category = ""
    if "J500" in name: category = "(J500)"
    if "J300" in name: category = "(J300)"
    elif "J200" in name: category = "(J200)"
    elif "J100" in name: category = "(J100)"
    elif "J60" in name: category = "(J60)"
    elif "J30" in name: category = "(J30)"
    elif "Australian Open" in name: category = "(Australian Open) üèÜ"
    elif "Roland Garros" in name: category = "(Roland Garros) üèÜ"
    elif "Wimbledon" in name: category = "(Wimbledon) üèÜ"
    elif "US Open" in name: category = "(US Open) üèÜ"
    
    main_name = name.replace(category, "").strip()
    main_name = re.sub(r',.*$', '', main_name).strip()
    
    return f"{main_name} {category}"

def get_tournament_name(soup):
    title_tag = soup.find('title')
    if title_tag:
        return clean_tournament_name(title_tag.text.split('|')[0].strip())
    return "TORNEO DESCONOCIDO"

def get_tournament_links(url):
    """Obtiene los enlaces de los torneos desde la p√°gina principal"""
    print(f"\nObteniendo enlaces de torneos de: {url}")
    
    try:
        response = SESSION.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        full_results_links = []
        for link in soup.find_all('a', class_='fullResults'):
            href = link.get('href')
            if href:
                full_link = href if href.startswith('http') else f"{BASE_URL}{href}"
                full_results_links.append(full_link)
        
        print(f"Se encontraron {len(full_results_links)} torneos")
        return full_results_links
    
    except Exception as e:
        print(f"Error al obtener enlaces de torneos: {str(e)}")
        return []

def extract_champion(soup):
    """Extrae a la campeona de la tabla de la final"""
    final_tables = soup.select('div.tabcontent table.round')
    if not final_tables:
        final_tables = soup.select('table.round')
    
    if not final_tables:
        return None
    
    final_table = final_tables[-1]
    
    # Buscar la fila del ganador (contiene td.winner)
    winner_row = None
    for row in final_table.select('tr'):
        if row.select_one('td.winner'):
            winner_row = row
            break
    
    if not winner_row:
        return None
    
    winner_cell = winner_row.select_one('td.winner a') or winner_row.select_one('td.player a')
    if not winner_cell:
        return None
    
    try:
        # Extraer nombre completo
        name_parts = winner_cell.get_text(strip=True).split(',')
        if len(name_parts) >= 2:
            last_name, first_name = [x.strip() for x in name_parts[:2]]
            full_name = f"{first_name} {last_name}"
        else:
            full_name = winner_cell.get_text(strip=True)
            last_name, first_name = full_name.split(' ', 1) if ' ' in full_name else (full_name, '')
        
        # Buscar la fila ORIGINAL del jugador para obtener su pa√≠s
        player_row = None
        for row in final_table.select('tr'):
            player_link = row.select_one('td.player a')
            if player_link:
                player_name = player_link.get_text(strip=True)
                if (player_name == f"{last_name}, {first_name}" or 
                    player_name == full_name or 
                    (first_name and player_name.startswith(last_name))):
                    player_row = row
                    break
        
        country_code = "UNK"
        if player_row:
            country_span = player_row.select_one('td.ctry')
            if country_span:
                match = re.search(r'\[([A-Z]{3})\]', country_span.get_text(strip=True))
                if match:
                    country_code = match.group(1)
                else:
                    # Intentar extraer el c√≥digo del pa√≠s de otra forma
                    country_img = country_span.find('img')
                    if country_img and 'title' in country_img.attrs:
                        country_title = country_img['title'].upper()
                        if len(country_title) == 3:
                            country_code = country_title
        
        # Obtener a√±o de nacimiento
        birth_year = None
        href = winner_cell.get('href', '')
        if href:
            if isinstance(href, list):
                href = href[0] if href else ''
            profile_url = f"{BASE_URL}{href}" if not href.startswith('http') else href
            
            try:
                profile_response = SESSION.get(profile_url, timeout=10)
                if profile_response.status_code == 200:
                    profile_soup = BeautifulSoup(profile_response.text, 'html.parser')
                    birth_text = profile_soup.get_text()
                    if "Born in" in birth_text:
                        match = re.search(r"Born in.*?(\d{4})", birth_text)
                        if match:
                            birth_year = match.group(1)
            except:
                pass
        
        return {
            'name': full_name,
            'country_code': country_code,
            'birth_year': birth_year or "N/A"
        }
    except Exception as e:
        print(f"Error extrayendo campeona: {str(e)}")
        return None

def process_tournament(url):
    """Procesa un torneo individual para extraer la campeona"""
    try:
        response = SESSION.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        tournament_name = get_tournament_name(soup)
        
        print(f"\nProcesando torneo: {tournament_name}")
        
        champion_data = extract_champion(soup)
        
        if champion_data:
            flag = get_country_flag(champion_data['country_code'])
            if champion_data['birth_year'] != "N/A":
                output_content = f"{flag} {champion_data['name']} [{champion_data['birth_year']}] {tournament_name.split()[-1]}"
            else:
                output_content = f"{flag} {champion_data['name']} {tournament_name.split()[-1]}"
            return output_content
        
        return None
        
    except Exception as e:
        print(f"Error procesando torneo: {str(e)}")
        return None

def get_month_from_url(url):
    """Extrae el mes de la URL"""
    match = re.search(r'date=(\d{4})(\d{2})', url)
    if match:
        year, month = int(match.group(1)), int(match.group(2))
        return MONTH_NAMES.get(month, f"Mes {month}")
    return "Fecha desconocida"

def main():
    print("Iniciando proceso completo...")
    
    # Diccionario para almacenar campeonas por mes y semana
    champions_data = {}
    
    for week_url in WEEKLY_URLS:
        month = get_month_from_url(week_url)
        if month not in champions_data:
            champions_data[month] = []
        
        print(f"\nProcesando semana: {week_url}")
        
        # Obtener enlaces de torneos para esta semana
        tournament_urls = get_tournament_links(week_url)
        
        if not tournament_urls:
            print(f"No se encontraron torneos para esta semana")
            champions_data[month].append([])  # Lista vac√≠a para mantener el orden
            continue
        
        # Procesar cada torneo de esta semana
        week_champions = []
        for tournament_url in tournament_urls:
            champion = process_tournament(tournament_url)
            if champion:
                week_champions.append(champion)
        
        champions_data[month].append(week_champions)
    
    # Generar el output organizado por meses y semanas
    all_output = ""
    for month, weeks in champions_data.items():
        if any(weeks):  # Solo agregar meses con datos
            all_output += f"\n{month}:\n\n"
            
            for week in weeks:
                if week:  # Solo agregar semanas con campeonas
                    all_output += "\n".join(week) + "\n\n"
    
    if all_output:
        filename = "campeonas_junior_semanales.txt"
        with open(filename, 'w', encoding='utf-8') as file:
            file.write(all_output)
        print(f"\nüéâ Resultados guardados en: {filename}")
        print("\nüìÑ Contenido del archivo:\n")
        print(all_output)
    else:
        print("\n‚ö†Ô∏è No se encontraron campeonas v√°lidas en los torneos procesados")

if __name__ == "__main__":
    main()